
%% bare_conf.tex
%% V1.4
%% 2012/12/27
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex,
%%                    bare_jrnl_transmag.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%\usepackage{amsmath,graphicx,subfigure, amssymb}
\usepackage{amsmath,graphicx,amssymb}
\usepackage[small]{caption}
\usepackage{subcaption}





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Augmediated Reality system based on 3D camera selfgesture sensing}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{AUTHOR} \and
\IEEEauthorblockN{AUTHOR} \and
\IEEEauthorblockN{AUTHOR}}
%\IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: http://www.michaelshell.org/contact.html}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
This paper presents a 3D hand gesture recognition system for use on a wearable computer system. The wearable system consists of a mobile computer, a head-mounted display and an infrared range camera mounted onto the head mounted display. In a surveillance setting, the camera is fixed to a property and oversees the environment. Whereas for souisveillance purpose, the range camera is worn by the user that shares the same view point of the user. The computer vision is benefitted by the use of the range camera to allow image segmentation by using both the infrared and depth information from the device for 3D hand gesture recognition system. The gesture recognition is then accomplished by using a neural network on the segmented hand. Recognized gestures are used to provide the user with interactions in a mediated reality environment.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\begin{figure}
\centering
\begin{subfigure}[b]{0.7\columnwidth}
\centering
\includegraphics[width=\columnwidth,]{figs/crop_wearable}
\caption{The augmented reality; gesture recognition wearable unit.}
\label{wearable}
\end{subfigure}
\\
\vspace{5pt}
\begin{subfigure}[b]{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{figs/crop_sample}
\caption{A user wearing the wearable unit.}
\label{user_wearable}
\end{subfigure}
\caption{The wearable unit consists of a ASUS Xtion (PrimeSense range camera), ODROID-X (mobile computer) and Epson Moverio (transparent display). This allows the user to experience a sense of augmented reality. The range camera is mounted onto the display and views the world from the user's point of view.}
\end{figure}

%\section{Introduction}
\section{Introduction}
In recent years, gestures have been incorporated in various mobile devices such as phones and tablets \cite{androidgestures,bbgeestures,bbgesturesz10}. Most of these devices rely on the multi-touch surface as their gesture interface. Other gesture recognition systems, such as the Microsoft Kinect, utilize an infrared range camera as the input device \cite{msprimesense2010press}, which enables the user with `hands-free' input (not needing to hold any devices), via gestures. However, these devices, whether it require phyiscal interaction or not, are usually external to the user, that is, the user interacts with the device from a third person perspective, with respect to the device. For example, consider the Microsoft Xbox Kinect, both the user and the Kinect can be considered separate entities in the interaction - once the user walks away from the Kinect, or the Kinect is powered off, there is no more interaction. Essentially, these devices we use in our everyday lives are not integrated with us.

The principle of Humanistic Intelligence (HI) \cite{mann2001wearable}, can be used to overcome this separation of user and device. That is, by using a wearable computer, there is no seperation of device and user - the user is part of the feedback to the device and the device is always ready to accept input of the user, regardless of time or space \cite{mann2002wearcam, mann2002wearable, mannaaai361, mistry2009sixthsense}. Therefore, by augmenting an HI wearable system with an infrared range camera, the user can gain `hands-free' natural interaction with the system via gestures.
%
%A wearable computer system would better integrated with us than external devices. There are wearable computer systems that integrate with us and gives a more natural interface for gestures \cite{mann2002wearcam, mann2002wearable, mannaaai361, mistry2009sixthsense}. However, some of these systems either do not allow for mediated reality or have a gesture interface or 3D data or have some specific requirements (such as the user wearing some accessory on their hand) for gesture recognition to work.
%%Recently, some researchers have proposed ***(a particular use?)*** wearable computers, such as a necklace worn camera, for use as a life logging system to record a person's life in a first person perspective \cite{mann2005designing}, \cite{mistry2009sixthsense}. In some of these devices, gesture recognition is proposed so the user can provide input to the wearable camera \cite{mistry2009sixthsense}. ???However, the user did not get any feed back from the camera to create a mediated reality system.
%
%In this project, we are proposing a 3D hand gesture recognition system for a wearable mediated reality system. Our recognition system uses an ASUS Xtion (PrimeSense based) range camera as an input device - this gives us 3D data and reduces the complexity of the recognition algorithm, .
%%We collect the frames from the sensor and process it with a wearable computer, ODROID-X, in real time to perform gesture recognition or any mediated reality environment. After recognizing the gestures we send the corresponding action back to Epson Moverio BT-100, which is a  transparent head mount display that runs on Android 2.2. Due to its transparent display, we are able to mediate the visible reality for the user.

\begin{figure*}
\centering
\includegraphics[width=2\columnwidth]{figs/train_flow3.pdf}
\caption{The masked images are cropped, resized and fed into the neural net to determine the gesture.}
\label{flow_diagram}
\end{figure*}
%Hand gesture recognition consists of two main components: hand detection and gesture recognition. Hand detection concerns about how to robustly determine the contour of the hand in an environment with complex background. To achieve this, many researchers take advantage of controlled environment such as constant lighting and static background \cite{imagawa1998color, hong2000gesture}. However, these methods are not reliable in real world environment with complex lighting and background changes. Other methods focus on tonal based features. Some focuses on the skin tone as a feature to perform segmentation \cite{kjeldsen1996toward}. These features are not reliable in certain background or lighting condition, for example, similar colours between the background and human skin. In addition, some methods use specially coloured gloves or other sensing device such as the data glove to provide additional information for segmentation \cite{sturman1994survey}. Understanding the problems of the methods discussed above, we explore an alternative method based on the depth information provided by the PrimeSense camera to perform close range hand detection. 
%
%In recent years, many researchers resort to the infrared range camera such as the PrimeSense camera. The device computes the depth map which contains information of an object's distance with respect to the viewing camera. The depth map can be considered as an additional dimension of information for feature extraction and image segmentation \cite{ren2011robust, uebersax2011real}. Instead of performing gesture recognition using PrimeSense camera from a third person view, where the camera observes user’s gestures on a steady platform \cite{li2009real}, we propose to use the camera from the first person perpective, where it is mounted on the user's eye glasses and observe the world from the user's point of view \cite{mann2011blind}.

\section{3D Hand Gesture Recognition}
Hand gesture recognition consists of two main components:
\begin{enumerate}
    \item Hand detection
    \item Gesture recognition
\end{enumerate}
Hand detection concerns about how to robustly determine the contour of the hand in an environment with complex background; while gesture recognition is concerned about correctly interpreting the meaning of a gesture.

To achieve hand detection, many researchers take advantage of controlled environments, such as constant lighting and static background \cite{imagawa1998color, hong2000gesture}. However, these methods are not reliable in real world environments with complex lighting and background changes. Other methods focus on tonal based features. Some focuses on the skin tone as a feature to perform segmentation \cite{kjeldsen1996toward}. These features are not reliable in certain background or lighting condition, for example, similar colours between the background and human skin. In addition, some methods use specially coloured gloves or other sensing device such as the data glove to provide additional information for segmentation \cite{sturman1994survey}. Understanding the problems of the methods discussed above, we explore an alternative method based on the depth information provided by an infrared range camera, such as a PrimeSense camera, to perform close range hand detection. %the PrimeSense camera to perform close range hand detection. 
Such a camera computes the depth map which contains information of an object's distance with respect to the camera. The depth map can be considered as an additional dimension of information for feature extraction and image segmentation \cite{ren2011robust, uebersax2011real}. Most of the current approaches use only an infrared range camera from a third person perspective. The solution assumes there is no confusion between the hand’s depth information with other obstacles in the environment. Besides the infrared range camera, some approaches use a combination of a single color camera, a stereo color camera and a thermal camera to obtain additional information for image processing and denoising\cite{appenrodt2010data}. These methods achieve promising results in the static indoor setting.

\subsection{Proposed Method}
For a mobile or a wearable platform, we attempt to minimize the number of devices in the system and instead of performing gesture recognition using PrimeSense camera from a third person view, where the camera observes the user’s gestures on a steady platform \cite{li2009real}, we propose to use the camera from the first person perspective, where it is mounted on the user's eye glasses and observe the world from the user's point of view \cite{mann2011blind}. Therefore, a wearable construct based on the PrimeSense camera is of interest, which has appeared in the use of the navigation helmet proposed by Steve et. al \cite{mann2011blind}.

Similar to methods \cite{li2009real, kjeldsen1996toward, ren2011robust, uebersax2011real}, we achieve the gesture recognition in two stages:
\begin{enumerate}
\item segmentation
\item classification
\end{enumerate}

The purpose of the segmentation stage is to first locate the hands of the user in the image. We apply the classfication algorithm to segmented image to identify the gestures.

\subsection{Segmentation}
In order for the system to classify the gesture, it needs to first locate the hands. Since the hands of the user are of interest, we assume the hands appear as objects within close proximity to the camera. This information can be obtained from the range camera sensor, like the PrimeSense camera. The PrimeSense camera provides two types of images:
\begin{enumerate}
\item Infrared image
\item Computed depth map
\end{enumerate}
The infrared image is a gray scale image that shows the level of infrared light sensed by the camera. The depth map is provided by the camera which approximates the distance of the objects in the scene. The two images are thresholded independently to filter out the pixels that do not meet the constraints. The results are two binary images that intersect to produce the final image mask, as shown in Figure \ref{image_segmentation}. The image mask is a binary image for hand extraction.

Due to device limitations, the depth map can only return a finite range of distance values. This is a known hardware specification. A depth map pixel is set to zero if the viewing object is either too close or too far from the camera. Additionally, the distance of any light source or reflective material in the scene that corrupts the projected pattern is unknown and set to zero. With the camera worn on the user's head, we assume that the gestures appear within the distance range up to the fully stretched arm length away from the camera. This means that objects with depth values under certain threshold $d_{th}$ are considered the candidates of the user's hand. However, this includes false candidates such as light sources, shadows, reflective objects, and distant objects. The resulting binary image sets the pixels under $d_{th}$ to one and others set to zero.

Since the PrimeSense camera projects the patterns in the infrared spectrum, given the condition that no other infrared light source is present, the objects closer to the camera are relatively brighter than the objects from afar. We assume this property even with other light sources or highly reflective materials are present in the scene. With this assumption, a binary image based on the infrared image is created by thresholding the pixel values. Denote $p_{th}$ as the pixel threshold, we set the pixels below $p_{th}$ to zero and others to one.

The intersection of the two binary images is performed to generate the mask. The binary image of the infrared image filters out the distant objects that would appear as candidates in the binary image of the depth map while the binary image of the depth map filters out the pixel intensities greater than $p_{th}$ that are too far from the camera, as shown in Figure \ref{image_segmentation}. 

To extract the hands from the image mask, we resort to fitting bounding boxes on the extracted contours. Typically, the two hands are the largest objects in the image mask. Therefore, we apply this heuristics of finding only the objects that are bounded by the two largest boxes. The two largest objects become the candidates for gesture recognition.

\begin{figure} \centering \includegraphics[width=0.8\columnwidth]{figs/crop_segmentation.pdf} \caption{Image segmentation example. The binary image on the left sets pixels to one if the depth map is unable to identify the object's relative distance. The binary image on the right filters out the lower than threshold pixels by setting them to zero. The intersection of the two binary images becomes the image mask for gesture recognition. Notice that there are still noises present in the image mask. This happens when both binary images do not filter out the out-of-range pixels. For example, a close distance bright light source is both unidentified in the depth map and is high in pixel values in the infrared image.}\label{image_segmentation} \end{figure}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/neural_net-crop.pdf}
\caption{The neural net implemented, takes 400 pixels at the input layer; has 100 nodes in the hidden layer; and 4 output nodes. Each node represents the confidence of the input being a specific gesture.}
\label{neural_net}
\end{figure}

\subsection{Classification}
We use a single layer neural network to achieve real time gesture recognition. The extracted image mask of the hands is resized to a $20\times20$ image. This image is fed into the neural network, and the neural network outputs the probability of each gesture. Each pixel in this image patch is treated as an input unit as shown in Figure \ref{neural_net}. Therefore, our input vectors to the neural network are always 400 to 1. For the hidden layer, we choose to only implement 100 hidden units. By choosing a small number for the hidden units, we are able to limit our total parameter size to 40400. We decided this number is an efficient use of memory for a real time recognition system. Finally, we have 4 output units since there are 4 different possible gestures we are interested in, as shown in Figure \ref{gestures}. Each of these output units is the probability of an unique gesture.
\begin{figure}
\centering
\includegraphics[width=0.95\columnwidth]{figs/gesturesv2-crop.pdf}
\caption{Sample masked images of the 4-gestures trained into the neural net. During the classfication of the gesture, the system will recognize the two gestures: point-angled and point-up as finger pointing. This helps increasing the flexibility of gesture recognition for the users to post gestures that are natural to them.}
\label{gestures}
\end{figure}

To train our neural network, we first need to define the cost function. This function is the log likelihood of logistic regression. To find the best possible parameters for the model, we suppose to find find the parameter which will maximize this function. However, due to our gradient descent setting, we decided to add a minus sign in front of it and make it a minimization problem. Therefore, we are trying to maximize the log likelihood function using minimization techniques. To prevent over fitting to the training data, we added a regularization term by adding the square of each parameter at the end of the cost function. These regularization terms will punish the cost function as the parameters become too big, which can result in a floating point overflow. The training cost function $J(\theta)$:
\begin{equation}
J(\theta) = l(\theta) + R(\theta, \lambda)
\end{equation}
\\
The term $l(\theta)$ is the logistic regression for minimization: 
\begin{equation}
\begin{split}
l(\theta) = -\frac{1}{s}\sum_{i=1}^{s} \sum_{j=1}^c 
& [y_j^{(i)}log(h_{\theta}(x^{(i)}))_j + \\
& (1-y_j^{(i)})log(1-(h_\theta(x^{(i)}))_j)]
\end{split}
\end{equation}
for which $s$ denotes the total number of training cases and $c$ denotes the total number of output gestures. Since our objective of this function is to add up the cost from each of our training cases. Thus, we use $i$ to denote the current training cases that are being used to calculate the cost. $h_{\theta}(x^{(i)})$ denotes the estimation resulted from the forward propagation. After calculating the the estimate from forward propagation, we use a logistic function to rescale that number between 0 and 1.
\\
The term $R(\theta, \lambda)$ is the regularization term: 
\begin{equation}
\begin{split}
R(\theta, \lambda) = \frac{\lambda}{2s}[ \sum_{i=1}^{n} \sum_{j=1}^{p} (\theta_{i,j}^{(1)})^2 + \sum_{i=1}^{c} \sum_{j=1}^{n} (\theta_{i,j}^{(2)})^2]
\end{split}
\end{equation}
for which $n$ denotes the total number of nodes in the hidden layer and $p$ denotes the total number of nodes in the input layer, which is also the number of pixel we have in each of our training image patch.

\subsection{Training}
The training data were collected using the PrimeSense camera to record a sequence of the image masks of various hand gestures. In particular, we focus on the following gestures:
\begin{itemize}
    \item the framing gestures (consists of both hands that form the corners in diagonal of each other)
    \item the finger pointing gesture.
\end{itemize}

\subsubsection{Gesture Variation}
One problem associated with gesture recognition is that the the orientation or form of a single gesture varies, with respect to the user and instance. 
%As discussed in \cite{ren2011robust, uebersax2011real, li2009real} recognizing gestures requires the system to account for the inconsistency.
Specifically, we consider two types of variations: the variations due to change in orientation \cite{ren2011robust, uebersax2011real, li2009real} and variations due to different forms of gesture that represent the same action. 

Figure \ref{gesture_data} shows some gestures that have the same meanings. The differences of these forms of gestures are not mere geometric transformation from one to another. To adapt to the form variations, we first define a group of different gestures that mean the same action. Each gesture of the same group is trained separately.

In addition to the form variations, we also attempt to train for the variations in orientation. This allows recognition system to adapt to slight angle changes of the hand. The inclusion of the variations helps the training to account for the gesture differences, which avoids limited recognition of only a single instance of the gesture.
\begin{figure} \centering \includegraphics[width=1\columnwidth]{figs/crop_training.pdf} \caption{Demonstration of the variations of gestures. The top two rows are one instance of the three different gestures: upper coner, lower corner and finger pointing. There are other possible gestures that represent the same action. The lower two rows are the examples of the alternative gesture of the top rows.}\label{gesture_data} \end{figure}
\begin{figure}
\centering
\begin{subfigure}[b]{\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figs/accur_vs_iter.pdf}
\end{subfigure}
\\
\vspace{10pt}
\begin{subfigure}[b]{\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figs/cost_vs_iter2.pdf}
\end{subfigure}
\caption{A graph of the Cost function versus Training iteration. The graph shows the iteration at which to stop training the neural net - the minimum point of the testing cost. Beyond this iteration, more training causes an increase in the testing cost. At that iteration, the training set achieves a 99.8\% accuracy and the testing set achieves 96.8\% accuracy.}
\label{stop_graph}
\end{figure}
\subsubsection{Data Collection}
Collecting more training data is one of the most effective way to improve the performance of a learning algorithm. In our setting, collecting more training data simply means recording more gesture samples  in our daily use of the device. Although we are achieving high accuracy on our existing training data. We are constantly streaming our gestures and give label them with the correct label. This data collection approach will keep improving our learning algorithm the more we use it.

\subsubsection{Early Stopping}
In order to avoid over fitting to our training data. We separated 80\% of our data as our training data and 20\% of our data as test data. On every iteration of neural net training, we run  forward propagation to get our gesture prediction accuracy and cost on both training and test set. We plot the cost on both training and test sets versus the number of training iterations as shown in Figure \ref{stop_graph}. As you can see in the Figure \ref{stop_graph}, at around iteration 2000, the cost of the test data starts to increase while the cost of the training data is still decreasing. This tells us that after approximately 2000 iterations, our neural net is being over trained to the training data, that is, if left to train forever, the neural network will only match items in the training data and reject everything else.

\section{Implementation}
In this project, our goal is to create and implement a mediated reality system using gesture recognition as a user interface. To achieve this, we utilize the ASUS Xtion sensor to observe the world and gestures from a first person view. The ASUS Xtion is a PrimeSense based range camera which provides depth maps and infrared images of the scene it is observing. This camera uses an infrared projector and infrared camera to determine the depth map. The images are processed in real time with an ODROID-X, which is an Android-based mobile development platform with a 1.7GHz ARM Cortex-A9 Quad Core processor. Finally we display the result using the Epson Moverio BT-100 to the user. 
%\subsection{Epson Moverio BT-100}
The Epson Moverio BT-100 is a transparent head mounted display that runs on Android 2.2. Based on the principles discussed in \cite{mann2001wearable}, Epson's Moverio is a good candidate for mediated reality applications due to its transparent displays .The Moverio is capable of streaming from an external NTSC source and was therefore used as a display for the processed information from the range camera. In this project, we processed the range camera information with ODROID-X and added additional mediated reality information to the Moverio. The user will see a mediated reality, such as a mediated user gesture interface that will interact with real world object.

\subsection{Performance}
The training stage of our neural network achieved an accuracy of 99.8\%. The cross-validation of the trained neural network achieved an accuracy of 96.8\%. The performance in frames-per-second (FPS) on the ODROID-X is 20 FPS.

\section{Application}

\subsection{First Person View Gesture Recognition Based User Interface}
By providing a user interface around our gesture recognition system along with the Epson glasses, we are able to transform the create a mediated reality system. Users are able to see the real world environment along with the user interface. As you can see in Figures \ref{select_off} and \ref{select_on}, the user is able to toggle a $Select$ function by overlaying his/her `finger pointing gesture' over the $Select$ button.  Unlike traditional devices, where the users have to control the device with their hands, our approach gives the user a more intuitive and hands free control over the device.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/28}
        \caption{$Select$ OFF}\label{select_off}
    \end{subfigure}~\begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/29}
        \caption{$Select$ ON}\label{select_on}
    \end{subfigure}
    \\
    \vspace{10pt}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/45}
        \caption{Initiating $Drag Me$}\label{drag_init}
    \end{subfigure}~\begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/46}
        \caption{Moving $Drag Me$}\label{drag_me}
    \end{subfigure}
    \caption{Some sample interactions using the wearable gesture recognition system. Fig \ref{select_off} and Fig \ref{select_on} shows a gesture toggling a virtual-button. Fig \ref{drag_init} and Fig \ref{drag_me} shows a gesture moving a virtual object in the scene.}
\end{figure}

\subsection{Mediated Reality Window Management}
Mediated reality window management is another important application for our gesture recognition system. Users will be able to drag around an augmented window as you can see in Figures \ref{drag_init} and \ref{drag_me}. This window can contain various information such as conference calling, web pages and GPS maps. This application allows user to do their daily desktop/laptop environment task directly in a first person mediated reality window.
\begin{figure}
\centering
\includegraphics[width=\columnwidth,]{figs/bounding_box.png} 
\caption{Example of the system recognizing the two-corner gesture to overlay a bounding box (blue frame) on top of the view. The blue circles indicate the location of the finger tips. An application of such gesture is to select the region of interest of the view.} \label{bounding}
\end{figure}

\subsection{Region of Interest Selection With Gesture}
In most photo/image based object recognition task, it is very hard for the computer to understand which object in a particular photo we are really classifying. To reduce the search area for object classfication, methods such as eye tracking \cite{schiessl2003eye} and hand gestures are considered a natural way for the user to inform the system where the user's focus is. For example, if the scene contains a chair and a table and the person wants to see the price of that chair, without any additional indication from the user, the recognition system may only attempt to retrieve the price information of both items and display them. With  gesture recognition enabled in the system, the user is able to guide the system of a specific object of interest. This is done by first constraining the area of the view using the bounding box to bring forth the region of interest. The user may naturally select the region of interest by posting two-corner gesture, as seen in Figure \ref{bounding}.

In addition to utilizing our gesture recognition system as a preprocessing tool for object recognition, we integrate the use of QR code to improve the accuracy of object recognition and to speed up recognition performance. QR code has been used extensively over the past few years in terms of advertising on mobile platforms due to its simplicity and speed. People can look at a poster and scan the QR code and get redirect to the event website. In our wearable gesture recognition setting, it provide a natural experience when working with QR code technology. A user can be walking down the aisle in a grocery store and acquire product information by scanning QR codes. To activate the QR code and get additional information on the products such as ratings, the user can perform a gesture command to segment out the QR code of the interested product. Once the QR code are segmented and recognize, our wearable system will send a request to an URL and display the corresponding information. 

\section{Social Implications}
Serendipitous gesture recognition on a mobile wearable device requires constant sensing of the environment. For example, the vision based wearable gesture recognition system [figure1] continuously processes every frame of incoming video so that it is ready to recognize any hand gesture when it occurs, and gives commands corresponding to the gesture it recognizes. In definition, this particular setting will be classified as a sousveillance system [ref], which is the opposite of surveillance, i.e. instead of a camera system mounted to a building, the camera is human-centric, and the world through this device is captured as a first person view. In the past several decades, mass media has a debate over the appropriate use of sousveillance devices such as SixSense [ref] and Google Glass. In one hand, some people see sousveillance devices as a deprivation of personal privacy that should be forbidden in any public area. On the other hand, others view such devices as a revolutionary personal assistance that can perform augment reality [ref] and mediated reality [ref] tasks, such as providing an interactive augmented/augmediated reality interactions on subject matter in view of a user as the user sees it in a first-person view. [ref]

The interactive mediated reality building [figure 2] is a social experiment and artistic in(ter)vention conducted by author S. Mann to address the awareness of the use of sousveillance devices. Photography and cameras are examples of sousveillance. As common as such practices and devices are, there are still occasions when photographers are harassed by security staff or police officials. For example, persons with sousveillance devices are unwelcome in some commercial establishments where numerous surveillance cameras are installed.  In this sense, surveillance is often the veillance of hypocrisy. To raise the awareness of problems with this one-sided veillance, the interactive mediated reality building was designed to take photos of the photographer as the photographer scans the QR (Quick Response) code installed on the building. As the photographer scans the QR code, he/she will see an augmented reality sign indicating there is no photography allowed of the building. However, in order to scan the QR code and see this “NO PHOTOGRAPHY” sign, the photographer must have already taken photos of the building. In return, the building will capture a photo of the photographer since he/she has violated the signs displayed by the building. 

[figure 2]

In the past, people have implemented wearable gesture recognition algorithms based on a processing RGB frames fed from the sousveillance devices. To perform gesture recognition, these devices has to process every RGB frame in order to detect if there is any gesture command given by the user. As an example, the SixSense project uses a RGB camera to capture user's gesture command by detecting the color finger marker taped on the user's fingers. Another good example will be the telepointer [ref], which feeds RGB frames in order to provide a visual collaborative experience to the user.  However, both projects requires constant polling on the RGB frame by the device, which has the potential capture sensitive information of other surrounding people or environment. Thus, it has the potential to cause privacy intrusion. 

[figure 1-1] [figure 1-2]
[figure 1-1]: the telepointer project utilized an RGB camera to capture the gesture in a grocery store environment.
[figure 1-2]: the SixSense project utilized a finger marker and RGB camera to capture the gestures in order to project augment reality frames on a surface.

To avoids these privacy concerns caused by RGB based wearable gesture recognition system, our system use an infrared (only) camera for gesture recognition.  Instead of polling from the RGB frame, we perform constant polling of the infrared frame. This approach not only provide promising gesture recognition accuracy, but also avoid capturing sensitive information during its operation[figure IR/RBG side by side]. 

A wearable computer with gesture interface "frees" our hands of devices and enables us to interact with the world in a more "natural" way. However, like current hand-held devices, it still requires at least one of our hands to be free.

In a wider scope of things, hand gestures is a form of expression and communications in our current world \ref{}. As each culture/country has its own spoken language, so too does each have its own set of hand gestures with their own meanings. Thus, if a wearable system is designed with a fixed set of gestures, it is possible that these gestures have different meanings in different cultures - some of which may even be derogatory. \ref{}

\section{Future Work}
In further development of the system, we are experimenting and expanding on our current base to incorporate more gestures to our system and create more ways for the user to interact with the environment in first person perspective. For example, we are currently developing a sport aid system that will help a pool player improve their skills. We are incorporating new gestures in this application to enable a user to segment the relevant ball and find the optimal path to hit the ball into the bag.

\section{Conclusion}
We have proposed a 3D hand gesture recognition wearable system utilizing the ASUS Xtion PrimeSense range camera. We process information from the range camera, in real time, to recognise hand gestures. Once we get the user input through the range camera, we display the interaction, such as the corresponding action due to a gesture, back to the user via the Epson Moverio BT-100. We trained a neural network learning algorithm to learn 4 different gestures (\ref{gestures}) that we are interested in to demonstrate the first prototype of our gesture interface and achieved 99.8\% training accuracy, 96.8\% testing accuracy, and we are able to run our recognition system at 20 FPS on a ODROID-X mobile computer.


% use section* for acknowledgement
\section*{Acknowledgment}
The Authors would like to thank Epson, ODROID and ASUS for their contributions to this project.



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

\bibliographystyle{IEEEtran}
\bibliography{3D_gesture}


% that's all folks
\end{document}


